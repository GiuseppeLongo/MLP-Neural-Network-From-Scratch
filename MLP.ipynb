{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron from scratch\n",
    "In this Notebook I detail how to build a multilayer perceptron for handwritten digit classification from scratch -- using only matrix multiplication libraries in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need [NumPy](https://www.numpy.org/) and [matplotlib](https://matplotlib.org/) if you intend to run this notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:15.682784Z",
     "start_time": "2019-07-15T22:16:15.559468Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "One of the first steps when working with data is almost always data preprocessing. However you come about your data, whether downloaded from an established dataset or created yourself, it's vital to carefully examine and prepare your data. Our data here is from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of `28x28` pixel images of handwritten digits from `0-9`, which has an important place in the history of machine learning. Some common questions to ask yourself when confronted with a new data set:\n",
    "1. What ranges of values does your data take? Do these values need to be rescaled?\n",
    "2. What shape is your data? Should each sample be a matrix ($n \\times m$), a vector ($1 \\times n$), or even a multi-dimensional tensor ($n \\times m \\times p \\times \\ldots$)?\n",
    "3. Are there fringe cases that need special attention?\n",
    "\n",
    "Our data lives in the directory `data` under the file names `mnist_train.csv` and `mnist_test.csv`, so the complete file path (from the main directory) are `data/mnist_train.csv` and `data/mnist_test.csv`. With that in mind, let's define a helper function `load_data` that we can use to quickly load in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:16.437154Z",
     "start_time": "2019-07-15T22:16:16.431149Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "    Function for loading MNIST data sored in comma delimited files. Labels for \n",
    "    each image are the first entry in each row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dit_name : str\n",
    "         Path to where data is contained\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        A (N x p=784) matrix of samples \n",
    "    Y : array_like\n",
    "        A (N x 1) matrix of labels for each sample\n",
    "    \"\"\"\n",
    "    data = list() # init a list called `data`\n",
    "    \n",
    "    with open(dir_name,\"r\") as f: # open the directory as a read (\"r\"), call it `f`\n",
    "        for line in f: # iterate through each `line` in `f`\n",
    "            split_line = np.array(line.split(',')) # split lines by `,` - cast the resultant list into an numpy array\n",
    "            split_line = split_line.astype(np.float32) # make the numpy array of str into floats\n",
    "            data.append(split_line) # collect the sample into the `data` list\n",
    "            \n",
    "    data = np.asarray(data) # convert the `data` list into a numpy array for easier indexing\n",
    "    \n",
    "    # as the first number in each sample is the label (0-9), extract that from the rest and return both (X,Y)\n",
    "    return data[:,1:],data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.705155Z",
     "start_time": "2019-07-15T20:09:44.341375Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,Y_train = load_data(\"data/mnist_train.csv\")\n",
    "X_test,Y_test = load_data(\"data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy to quickly get an idea for how your data looks is to examine the shape of the matrix it's stored using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `60000 x 784`, which tells us there are `60000` samples (images) each with dimension `784`. Each sample, typically presented as a 28 x 28 image, is unrolled into a 1-dimensional vector 28 x 28 = 784 contained within each row of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.720141Z",
     "start_time": "2019-07-15T20:10:03.717400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: 60000 x 784\n",
      "The shape of the test set is: 10000 x 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
    "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take at how the samples are represented, we can do this by calling `Y_train[index]` and `X_train[index]` (here I choose `index=0` to look at the very first sample). We first notice `Y_train[0]=5.0`, indicating this is the digit `5`. We confirm this later by visualizing some of these samples. We then notice each entry is an integer (cast into `np.float32` in our `load_data` function) ranging from `0-255`. This representation is common among image data, the numerical entries are pixel intensities typically shown in gray-scale ranging between `0` (black) and `255` (white). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.766216Z",
     "start_time": "2019-07-15T20:10:03.734027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "         18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "        172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "        253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "        249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "        250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "        221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "          2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "        253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "        172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "        132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "Y_train[index], X_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is nothing in principle wrong with this `0-255` representation, the value of `255` being the maximum is specific to images and somewhat an arbitrary choice in our context. Frequently people opt to rescale their data in these situations to range between `0-1`, which will be have some nice mathematical properties for us later. Of course, this can be done by simply diving each entry in `X_train` and `X_test` by its maximum value (accessed using `X_train.max()`). \n",
    "\n",
    "Further, each label in `Y_train` and `Y_test` are currently integers (e.g. `5.0` or `2.0`). For categorical data (each image must be categorized as label between `0-9`) we opt for a one-hot encoded representation of our data. What this means is each label is converted into a binary vector (e.g. the label 2.0 would be converted to the vector `[0,0,1,0,0,0,0,0,0,0]` and the label 9.0 would be the vector `[0,0,0,0,0,0,0,0,0,1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.883943Z",
     "start_time": "2019-07-15T20:10:03.780726Z"
    }
   },
   "outputs": [],
   "source": [
    "# rescale data between 0 - 1.0\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n",
    "\n",
    "# one-hot encode train (y_train) and test (y_test) set labels\n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size),Y_train.astype(np.int)] = 1.0\n",
    "\n",
    "y_test = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "y_test[np.arange(Y_test.size),Y_test.astype(np.int)] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's go ahead and visualize some of the inputs and their associated labels. We can do this using the `imshow` function (making sure to resize the flattened size 784 representation into a 28 x 28 matrix for compatability with the `imshow` function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:04.313194Z",
     "start_time": "2019-07-15T20:10:03.897501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 4\n",
    "fig,axes = plt.subplots(1,num_images,figsize=(15,10))\n",
    "for image,label,ax in zip(X_train[:num_images],y_train[:num_images],axes):\n",
    "    ax.imshow(image.reshape(28,28),cmap='gray',vmin=0,vmax=1.0)\n",
    "    ax.set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the math \n",
    "Here review some of the math underlying the multilayer perceptron (MLP) and the backpropagation algorithm. The MLP and backpropagation are central to understanding deep learning as a whole. Full stop. What I'm presenting here is by any means *not* an exhaustive exposition of the subject, and I **highly** recommend reading (at least) [chapters 1 & 2 of the free online book: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) for a more complete discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is a multilayer perceptron? Before we answer that question, let's dissect just a singular component: the perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:44:16.088920Z",
     "start_time": "2019-07-15T20:44:15.924981Z"
    }
   },
   "source": [
    "![Perceptron](imgs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image above represents schematically the conventional perceptron. Each input, represented as a multidimensional vector $\\mathbf{x}=\\{x_0=1,x_1,x_2,\\ldots,x_n\\}$, is multiplied by a set of weights $\\mathbf{w} = \\{w_0,w_1,w_2,\\ldots,w_n\\}$ to produce the weighted sum $$z = \\sum_{i=0}^{n} x_i w_i$$. In the case of a vanilla perceptron, this weighted sum $z$ is lastly fed through as step function to produce the predicted output $a = \\text{step}(z)$. This results in the output where $a=1$ if $z>0$ and $a=0$ if $z<0$.\n",
    "\n",
    "With the output $a$ taking the value 0 or 1, this type of structure really only works for binary classification problems (i.e. situations where each input $\\mathbf{x}$ has one of two intended labels: 0 or 1). The weights $\\mathbf{w}$ in this context can be interpreted as assigning credence to the different inputs $x_i$ based on their relative importance. Notice, there is an additional constant $x_0=1$ concatenated to each input, frequently called the *bias term* or just *the bias*. As there is still an associated weight $w_0$ along with this term, this effectively acts as an offset to the origin for the classification. In other words, think of the perceptron classification boundary as a multidimensional line (or plane) $y = m * x + b$. Inputs on one side of the line where $y<0$ get classified as one label ($a=0$), while  inputs on the other side of the line where $y>0$ are classified as the other label ($a=1$). In this geometric interpretation of the perceptron, the weights are like the slope (or vector perpendicular to the plane in higher dimensions) $\\mathbf{w} \\sim m$, and the bias is like the $b$ term $b \\sim w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the perceptron to the multilayer perceptron (MLP) simply introduces more intermediate weighted additions (layers) before the output and generalizes the step function used for binary classification to a broader class of functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](imgs/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neural networks are often schematically represented with a series if nodes and arrows as shown on the right. Each node represents one input, for example the input layer here is shows with 3 inputs, and each arrow represents a *unique* weight given to that particular input in the weighted sum (right side of figure). Note, in our application to MNIST each input would be `784+1` dimensional (including the bias) and the output `10` dimensional for representing the one-hot encoded binary vector of output labels. Rather than restricting ourselves to treating each sum with a step function as in the vanilla perceptron, this concept is generalized to any arbitrary function typically called the *activation function*, depicted as $\\sigma(\\cdot)$ in the schematic. In principle almost any function may be an activation function, however people typically use one of the following activation functions due to some nice mathematical properties they have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation_func](imgs/activation_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:29:56.750472Z",
     "start_time": "2019-07-15T21:29:56.746122Z"
    }
   },
   "source": [
    "Image credit: https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional layers between the inputs an outputs are typically called *hidden layers* because the particular value of the nodes within these layers are hidden to the user. A natural question to ask here is why are hidden layers helpful/necessary for deep learning? The answer is, shockingly, that they aren't necessary, but are very helpful. There exists a theorem called the \"Universal Approximation Theorem\" which states basically that a neural network with a single layer can approximate *any* non-linear function to arbitrary accuracy. While this may imply that only a single hidden layer is, in principle, sufficient for any problem, the necessary dimension of this hidden layer may become intractably large for some problems. Introducing many hidden layers allows your neural network to learn a hierarchy of concepts in the structure of the data. What this amounts to is in a MLP the earlier layers (i.e. the layers closer to the input) learn a coarser representation of the data and begins to more closely reflect the output representation nearer to the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deep](imgs/deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:15:42.928623Z",
     "start_time": "2019-07-15T22:15:42.799116Z"
    }
   },
   "source": [
    "Image credit: UChicago STAT 37710 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward\n",
    "Given a predefined neural network *architecture* (the *architecture* of a neural network refers to all the elements necessary to completely define the flow of data, which involve the number and size of hidden layers, which activation functions, the output size, etc.) the process of generating an output from an input is called a *forward* pass. As we shall see, for an MLP the forward pass may be succinctly represented as a series of matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:19.367434Z",
     "start_time": "2019-07-15T22:16:19.254509Z"
    }
   },
   "source": [
    "![MLP_big](imgs/MLP_big.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:35.149302Z",
     "start_time": "2019-07-15T22:16:35.038196Z"
    }
   },
   "source": [
    "Image credit: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the MLP represented schematically above with sigmoid activations $\\sigma$ in the hidden layer. Each neuron in the hidden layers will be weighted sums of the inputs: $$a_j = \\sigma(\\sum_{i=1}^{784} x_i* w_{i,j})$$ for $j=\\{1,2,\\ldots,15\\}$ (notice, there is no bias term in this example). From here it clear to see that the weights $w_{i,j}$ may be compacted into a matrix $W \\in {\\rm I\\!R}^{784\\times15}$ where $W_{i,j} = w_{i,j}$, allowing for all the neurons in the hidden layer to be efficiently calculated using matrix multiplication: $\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) = \\sigma(\\mathbf{x}W)$. Where $\\mathbf{a}^{(i)},\\mathbf{z}^{(i)} \\in {\\rm I\\!R}^{1\\times 15}$, with the superscript $(i)$ indicating the assocaited layer number, and $\\mathbf{x} \\in {\\rm I\\!R}^{1\\times 784}$ are both arranged as column vectors. Another, different, weight matrix is needed to transform the hidden layer to the output layer. Let's demarcate these two as $W^{(1)}$ for the matrix which transforms the inputs to the hidden layer $\\mathbf{a}^{(1)}$ and $W^{(2)} \\in {\\rm I\\!R}^{15\\times 10}$ for transforming the hidden layer to the output layer $\\mathbf{z}^{(2)}$. Mathematically, $$\\mathbf{z}^{(2)} = \\sigma(\\mathbf{x}W^{(1)})W^{(2)}$$. Notice, we've yet to treat this output $\\mathbf{z}^{(2)}$ with an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:32:36.274844Z",
     "start_time": "2019-07-15T22:32:36.261999Z"
    }
   },
   "source": [
    "The last step of process for classification tasks is actually producing a predictions from these numbers in the output layer $\\mathbf{z}^{(2)}$. Typically, in the case of multi-label classification, this is done using a *softmax* activation function which effectively converts the output neurons into probabilities. This has the mathematical form, $$\\text{softmax}(\\mathbf{z})_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{K=9}\\exp{(z_k)}}$$. The softmax activation function has the property of out outputs summing to 1 $\\sum_{k=1}^{K=9}\\text{softmax}(\\mathbf{z}^{(2)})_k = 1$, allowing each output $\\mathbf{a}^{(2)}_i = \\text{softmax}(\\mathbf{z}^{(2)})_i$ to be interpreted as the probability that the input is actually a digit `0-9`. Note, the output of a MLP does not need to have a softmax activation, for example in a regression setting a softmax activation would not make much sense. When evaluating the classification accuracy of the neural network, the input is typically classified according to the output label with the highest probability, $$\\text{prediction}(\\mathbf{x}) = \\text{argmax}\\  \\text{softmax}(\\mathbf{z}^{(2)}) = \\text{argmax}\\  \\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Now that we have an understanding of how an MLP generates outputs from inputs, we engage the problem of how to actually *train* this neural network. *Training* a neural network (in a supervised setting, which means each input comes with a known output) refers to the process of iteratively updating the weights of the network to improve it's performance. The performance of the neural network is evaluated using a *loss function* which quantitatively measures how \"close\" the neural network output is to the true output. In short, using *backpropagation* we aim to minimize the loss function with respect to the weights (also called *parameters*) of the neural network.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define precisely what we mean by a *cost function*. The cost function $C$ should reflect whatever objective your aim is for the neural network. Here, we're interested in having our neural network predict a digit label `0-9` for each input image. A naturally, and very reasonable, choice in many settings is simply using the L2 norm between our predicted and true outputs $$C = \\frac{1}{2N}\\sum_{i=1}^N||y_i - \\mathbf{a}^{(L)}(\\mathbf{x}_i)||^2$$, where $y_i$ represents the true label corresponding to input $\\mathbf{x}_i$ and $\\mathbf{a}^{(L)}(\\mathbf{x}_i)$ is the output of the neural network with $L$ layers taking $\\mathbf{x}_i$ as input. Note, in our previous example $\\mathbf{a}^{(L=2)}(\\mathbf{x}_i) = \\text{softmax}((\\sigma(\\mathbf{x}_iW^{(1)})W^{(2)}))$. Of course, when minimizing/maximizing a function any constant multiplies have no effect on the optimum which allows us to include the $\\frac{1}{2N}$ as a mathematical convenience whose purpose will be clear shortly (spoiler, it has to do with derivatives). We may equivalently write this cost function in terms of the cost incurred by each sample individually as $$C = \\frac{1}{N}\\sum_{i=1}^N C_i$$. Where the cost of an individual training sample is of course, in the case of our quadratic cost function, $C_i = \\frac{1}{2}||y_i - \\mathbf{a}^{(L)}(\\mathbf{x}_i)||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, training our neural network by incrementally adjusting the weights amounts updating each weight $W^{(l)}_{i,j}$ based on its influence to the cost function. This can be done with the *gradient descent* algorithm that defines the update rule $$W^{(l)}_{i,j} \\leftarrow W^{(l)}_{i,j} - \\alpha \\frac{\\partial C}{\\partial W^{(l)}_{i,j}}$$, where $\\alpha$ is a tunable parameter called the *learning rate*. Notice, vanilla gradient descent requires computing the derivative (people in the ML community use the word gradient, rather than derivative) $\\frac{\\partial C}{\\partial W^{(l)}_{i,j}}$ with respect to **all** the training samples. Of course, this can become expensive very quickly as some datasets, for example, contain millions of image and it's simply intractable to work with all that data at once. Therefore, people typically (read, always) employ a modified version of gradient descent called *stochastic gradient descent* (SGD) which updates the weights in *batches* (also called *mini batches*) of size $m$ $$W^{(l)}_{i,j} \\leftarrow W^{(l)}_{i,j} - \\frac{\\alpha}{m}\\sum_{k \\in \\text{batch}} \\frac{\\partial C_k}{\\partial W^{(l)}_{i,j}}$$. Where $C_i$ is the previously discussed cost function for an individual training sample $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually perform SGD we are left with the last of evaluating the gradient $\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}}$. However, we must be able to evaluate this gradient quickly and ideally in parallel as this calculation will be performed many, many times. This, finally, is where the *backpropagation* algorithm comes into play. \n",
    "\n",
    "Let's define the quantity $$\\delta^{(l)}_j = \\frac{\\partial C}{\\partial z^{(l)}_j}$$. Recalling that each hidden layer is expressed as $a^{(l)}_i = \\sigma(z^{(l)}_i) = \\sigma(\\sum_j W^{(l)}_{i,j} * a^{(l-1)}_j)$. Note, for simplicity I've omitted the bias term from $z$, and am using $\\partial C$ rather than $\\partial C_i$ for notational convenience. This $\\delta^{(l)}_j$ term may be thought of as the output error for neuron $j$ in layer $l$ with respect to the cost function $C$. Using $\\delta^{(l)}_j$ we can get at our sought after gradient by applying the chain rule $$\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}} = \\frac{\\partial C}{\\partial z^{(l)}_i} \\frac{\\partial z^{(l)}_i}{\\partial W^{(l)}_{i,j}} = \\delta^{(l)}_i * a^{(l-1)}_j$$. Where $\\frac{\\partial z^{(l)}_j}{\\partial W^{(l)}_{i,j}} = a^{(l-1)}$ can be readily derived recalling that $z^{(l)}_i = \\sum_k W^{(l)}_{i,k} * a^{(l-1)}_k$.\n",
    "\n",
    "Excellent! Now we have a closed-form analytical expression for $\\frac{\\partial C_i}{\\partial W_{i,j}}$. But how do we actually compute $\\delta^{(l)}_j$ in this expression? Let's start with the last layer $L$ and apply the chain rule again, $$\\delta^{(L)}_j = \\frac{\\partial C}{\\partial a^{(L)}_j} \\frac{\\partial a^{(L)}_j}{\\partial z^{(L)}_j} = \\frac{\\partial C}{\\partial a^{(L)}_j} \\sigma^{\\prime}(z^{(L)}_j)$$. The first term $\\frac{\\partial C}{\\partial a^{(L)}_j}$ is simply the error in the cost function with respect to the last layer, a quantity which can frequently be directly derived. For instance, in the case of the quadratic cost function $C = \\frac{1}{2}||y - a^{(L)}_{j}||^2$, the derivative is simply $\\frac{\\partial C}{\\partial a^{(L)}_j} = (a^{(L)}_j - y)$. Next, the $\\sigma^{\\prime}(z^{(L)}_j)$ is nothing more than the derivative of the activation function at numerical value of $z^{(L)}_j$. This is where the previously mentioned mathematical convenience of select activation functions comes in, say, for the sigmoid activation: $\\sigma^{\\prime}(x) = \\sigma(x) (1-\\sigma(x))$. What this means is that once we have computed $\\sigma(z^{(l)}_j)$ (for any $l$) during the forward pass of the network, we can store and reuse that value to efficiently compute $\\sigma^{\\prime}(z^{(l)}_j)$ during backpropagation!\n",
    "\n",
    "Okay, this is the last step. With the error in the last layer $\\delta^{(L)}_j$ computed final piece of the puzzle is computing the error for the remaining layers $\\delta^{(l)}_j$. Here, I will simply state the formula for the sake of brevity -- but I encourage you to look at the proof available [here](http://neuralnetworksanddeeplearning.com/chap2.html), or even better, try to prove it yourself! $$\\boldsymbol{\\delta}^{l}=((W^{(l+1)})^{T} \\boldsymbol{\\delta}^{l+1}) \\odot \\sigma^{\\prime}(\\mathbf{z}^{l})$$ The only new notation introduced here is the *hadamard product* $\\odot$: which simply performs an element-wise multiplication along two vectors (e.g. $(\\mathbf{x} \\odot \\mathbf{y})_i = x_i * y_i$). Looking at this equation, we realize now why backpropagation is called *backpropagation*: the errors for a given layer $\\boldsymbol{\\delta}^{l}$ depend on the errors in the following layer $\\boldsymbol{\\delta}^{l+1}$. Thus, once we have computed the error in the last layer $\\boldsymbol{\\delta}^{L}$, backpropagation effectively works backward to compute the errors in the remaining layers which are then purposed to the gradients $\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}}$ used to update the weights $W^{(l)}_{i,j}$ in SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the code\n",
    "Now that we have a handle on the math behind the MLP, we can now go ahead and implement everything we've looked at in code. Of course, we want to write efficient and fast code, which when writing in Python mean using matrix (NumPy multidimensional arrays) operations whenever possible. This mindset is generally good practice when writing trying to write efficient code in Python, as NumPy will automatically employ thread parallelism when working with NumPy arrays which will help substantially to accelerate our code.\n",
    "\n",
    "We'll first go through and write the code for each piece of an MLP in generic Python functions. We'll then wrap everything in an `MLP` class, which will allow us to easily access all the MLP functionality in a user friendly manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the forward pass of the neural network is relatively simply as everything is basically just a series of matrix multiplicaitons. The example we looked at when working through the math had a forward pass which took the form $$\\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$. We'll of course want to make things a bit more modular by allowing the user to choose some of the neural network paramters, such as the number of hidden layers `N_l` and the nunber of neurons per layer `L`. For simplicity we will restrict that all the hidden layers are treated with a sigmoid activaitons and the final layer processed with a softmax activation, which if you recall will be nessesary for us to ultimately make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by simply defining our activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of `x`, calculated element-wise\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float or array_like\n",
    "        input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid(x) : float or array_like\n",
    "        sigmoid applied to `x` element-wise\n",
    "    \"\"\"\n",
    "    return 1./(1.+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of `x`,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        (N x dim) array with N samples by p dimensions. dim=10 for MNIST classification. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    softmax(x) : float or array_like\n",
    "        softmax applied to `x` along the first axis.\n",
    "    \"\"\"\n",
    "    exponent = np.exp(x) # only compute the exponent once\n",
    "    return exponent/exponent.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs `x`, weight matricies `w`, and activations are in principle all we need to define the forward pass; however, for efficency reasons we'll want to store the outputs of the hidden layer neurons when performing the forward pass. Storing these values will help us later more quickly calcaulte the gradients during the backward pass. The `init_layers` functions will initalize these hidden layers as NumPy arrays, doing this before we begin training will help us save some overhead we would otherwise inccur reinitalizing these hidden layers before each forward pass. These hidden layer values will be stored in multi-dimensional matricies, called *tensors*. One dimension of these tensors will be the `batch size` which will indicate the number of samples simultaneously passed to MLP during one training loop (feed forward + backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(batch_size,layer_sizes):\n",
    "        \"\"\"\n",
    "        Initalize arrays to store the hidden layer ouputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Number of samples to concurrently feed through the network.\n",
    "        layer_sizes : array_like\n",
    "            Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden_layers : list\n",
    "            List of empty arrays used to hold hidden layer outputs. \n",
    "        \"\"\" \n",
    "        hidden_layers = [np.empty((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "        return hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform we forward pass our input `x` is consequtively by weight matricies and passed through with these activaiton functions. The paramters in these weight matricies will ultimately be learned through backpropagation, but each weight matrix must first be initalized to random values. There are a number of different methods for doing, but for the moment we'll use a simple approach of just drawing the numerical values from a normal distribution with mean zero and standard deviation 1. We could have also reasonably choosen to simply draw from a uniform distribution on the range `[-1,1]` (why would it be wrong to initalize the weight matricies with all zeros?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer_sizes):\n",
    "        \"\"\"\n",
    "        Initalize the paramters of the weight matricies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_sizes : array_like\n",
    "            Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Randomly initalized weight matricies based on the layer sizes. \n",
    "        \"\"\"\n",
    "        weights = list()\n",
    "        for i in range(layer_sizes.shape[0]-1):\n",
    "            weights.append(np.random.uniform(-1,1,size=[layer_sizes[i],layer_sizes[i+1]]))\n",
    "        weights = asarray(self.weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the `feed_forward` function to iterate though the calculations to perform the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(batch,hidden_layers,weights):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : array_like\n",
    "        (batch_size x dim) matrix of inputs\n",
    "    hidden_layers : list\n",
    "        List of hidden layer outputs\n",
    "    weights : array_like\n",
    "        Array of weight matricies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : array_like\n",
    "        Forward pass output of the MLP\n",
    "    hidden_layers : array_like\n",
    "        List of hidden layer outputs, populated from the forward pass.\n",
    "    \"\"\"\n",
    "    h_l = batch\n",
    "    hidden_layers[0] = h_l\n",
    "    for i,weight in enumerate(weights):\n",
    "        h_l = sigmoid(h_l.dot(weight))\n",
    "        hidden_layers[i+1]=h_l\n",
    "    output = softmax(hidden_layers[-1])\n",
    "    return output, hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalizing things prior to the forward pass makes much of the backward pass implimentation actually quite simple. For convinience we'll define a `sigmoid_prime` function, which simply computes the derivative of the sigmoid activation $\\sigma^{\\prime}$. We'll use this when computing the gradients during the backward pass. Recall, $\\sigma^{\\prime}(x) = \\sigma(x)(1-\\sigma(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(sigmoid_out):\n",
    "    \"\"\"\n",
    "    Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigmoid_out : array_like\n",
    "        Output values processed by a sigmoid function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_prime(h) : array_like\n",
    "        Derivative of sigmoid, based on value of sigmoid.\n",
    "    \"\"\"\n",
    "    return h*(1-h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything else in place computing we're finally ready to write the backpropagation algorithm. Again, the primary goal of this step is to update the parameters of the weight matricies using stochastic gradient descent on batches of training samples, measuring the error comparing the neural network outputs with the true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(output,batch_y,hidden_layers,weights,batch_size,lr):\n",
    "        \"\"\"\n",
    "        Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : array_like\n",
    "            Forward pass output of the MLP\n",
    "        batch_y : array_like\n",
    "            True labels for the samples in the batch\n",
    "        hidden_layers : list\n",
    "            List of hidden layer outputs  \n",
    "        weights : array_like\n",
    "            Array of weight matricies\n",
    "        lr : float\n",
    "            Learning rate for SGD\n",
    "        batch_size : int\n",
    "            Size of a training mini-batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Array of weight matricies, updated from the backpropagation.\n",
    "    \n",
    "        \"\"\"\n",
    "        delta_t = (output - batch_y)*sigmoid_prime(hidden_layers[-1])\n",
    "        for i in range(1,len(weights)+1):\n",
    "            weights[-i]-=lr*(hidden_layers[-i-1].T.dot(delta_t))/batch_size\n",
    "            delta_t = sigmoid_prime(hidden_layers[-i-1])*(delta_t.dot(weights[-i].T))\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined all the functions required to actually train the neural network, we're finally ready to assimilate everything into a training loop. The number of `epochs` we set defines the number of iterations we train our model. In each `epoch` a number of forward + backward pass are iteratively performed on mini-batches of data until we see all the training data.\n",
    "\n",
    "A number of new functions such as `loss`, `accuracy`, and `to_categorical` are used in the `train` function below. These are used to record some metrics we'll display during learning and will be defined in full in our final implimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,layer_sizes,batch_size=8,epochs=25,lr=1.0):\n",
    "    \"\"\"\n",
    "    Train the MLP.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        Forward pass output of the MLP\n",
    "    Y : array_like\n",
    "        True labels for the samples in the batch\n",
    "    layer_sizes : \n",
    "        Array of length `N_l`. Each entry is the number of neurons in each layer\n",
    "    batch_size : int\n",
    "        Size of a training mini-batch\n",
    "    epochs : int\n",
    "        Number of iterations to train for\n",
    "    lr : float\n",
    "        Learning rate for SGD\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : array_like\n",
    "        Array of weight matricies, updated from the backpropagation.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    hidden_layers = init_layers(batch_size,layer_sizes)\n",
    "    weights = init_weights(layer_sizes)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        shuffle = np.random.permutation(n_samples)       \n",
    "        X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n",
    "        Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "            output,hidden_layers = feed_forward(batch_x,hidden_layers,weights)  \n",
    "            train_loss += loss(output,batch_y)\n",
    "            train_acc += accuracy(to_categorical(output),batch_y)\n",
    "            weights = back_prop(output,batch_y,hidden_layers,weights,batch_size,lr)\n",
    "\n",
    "        train_loss = (train_loss/len(X_batches))\n",
    "        train_acc = (train_acc/len(X_batches))\n",
    "\n",
    "        train_time = round(time.time()-start,3)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron: final form\n",
    "Everything that's been outlined suffices for running the MLP. However, as it stands things are still quite clunkly. We can clean everything up by wrapping all this code into a Python Class. If you're unfamiliar with object oriented programming in Python, I highly reccomend checking out this tutorial. \n",
    "\n",
    "Our original function defintions have been slightly changed to help absuse some of the properties of having a class structure. A number of new functions, such as `predict` and `evaluate`, have been defined in the `MLP` class. Carefully go through this code and try to understand exactly what's being done and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = Y\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = Y_val\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        # VCompute the sigmoid\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's give this a try. Let's create a really simple MLP with only a single hidden layer `L=1` with 128 neurons `N_l=128`. We'll train with a `batch_size=8` for `epochs=25` and a learning rate `lr=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(X_train,Y_train,X_val,Y_val,L=1,N_l=128)\n",
    "model.train(batch_size=8,epochs=25,lr=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, along with the training data, `X_train` and `Y_train`, we include a validation dataset, `X_val` and `Y_val`. The purpose of this data is to evaluate the generizability of our model. We expect our model to perform well on our training data, because of course the objective of our optimization is to minimize the error with respect to the training data, but we'd like our model to generalize to new, never before seen, data. We therefore evaluate the accuracy and loss on a hold-out set which the model never sees during training. If the performance is good on this hold-out set we can be confident that our model is generalizing well, meaning we've mananged to generally teach a computer to read our hand-writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(model.train_loss,label=\"Train loss\")\n",
    "ax[0].plot(model.val_loss,label=\"Val loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(model.train_acc,label=\"Train acc\")\n",
    "ax[1].plot(model.val_acc,label=\"Val acc\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the results here are impressive, the *hyperparamters* were carefully currated to achieve this good performance. The *hyperparamters* reffer to the paramters of the model which are not learned during the training loop (e.g. the `batch_size`, learning rate `lr`, number of hidden layers `N_l`, number of neurons-per-layer `L`). In general, these paramters need to be carefully tuned for each problem. This tuning process tpyically involves training a model to completion under a number of differenty hyperparamter settings and selecting the combination of hyperparamters which yields the highest performance (measured in this case by the accuracy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
