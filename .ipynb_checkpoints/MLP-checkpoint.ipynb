{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron from scratch\n",
    "In this Notebook I detail how to build a multilayer perceptron for handwritten digit classification from scratch -- using only matrix multiplication libraries in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need [NumPy](https://www.numpy.org/) and [matplotlib](https://matplotlib.org/) if you intend to run this notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:15.682784Z",
     "start_time": "2019-07-15T22:16:15.559468Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "One of the first steps when working with data is almost always data preprocessing. However you come about your data, whether downloaded from an established dataset or created yourself, it's vital to carefully examine and prepare your data. Our data here is from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of `28x28` pixel images of handwritten digits from `0-9`, which has an important place in the history of machine learning. Some common questions to ask yourself when confronted with a new data set:\n",
    "1. What ranges of values does your data take? Do these values need to be rescaled?\n",
    "2. What shape is your data? Should each sample be a matrix ($n \\times m$), a vector ($1 \\times n$), or even a multi-dimensional tensor ($n \\times m \\times p \\times \\ldots$)?\n",
    "3. Are there fringe cases that need special attention?\n",
    "\n",
    "Our data lives in the directory `data` under the file names `mnist_train.csv` and `mnist_test.csv`, so the complete file path (from the main directory) are `data/mnist_train.csv` and `data/mnist_test.csv`. With that in mind, let's define a helper function `load_data` that we can use to quickly load in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:16.437154Z",
     "start_time": "2019-07-15T22:16:16.431149Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "    Function for loading MNIST data sored in comma delimited files. Labels for \n",
    "    each image are the first entry in each row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dit_name : str\n",
    "         Path to where data is contained\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        A (N x p=784) matrix of samples \n",
    "    Y : array_like\n",
    "        A (N x 1) matrix of labels for each sample\n",
    "    \"\"\"\n",
    "    data = list() # init a list called `data`\n",
    "    \n",
    "    with open(dir_name,\"r\") as f: # open the directory as a read (\"r\"), call it `f`\n",
    "        for line in f: # iterate through each `line` in `f`\n",
    "            split_line = np.array(line.split(',')) # split lines by `,` - cast the resultant list into an numpy array\n",
    "            split_line = split_line.astype(np.float32) # make the numpy array of str into floats\n",
    "            data.append(split_line) # collect the sample into the `data` list\n",
    "            \n",
    "    data = np.asarray(data) # convert the `data` list into a numpy array for easier indexing\n",
    "    \n",
    "    # as the first number in each sample is the label (0-9), extract that from the rest and return both (X,Y)\n",
    "    return data[:,1:],data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.705155Z",
     "start_time": "2019-07-15T20:09:44.341375Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,Y_train = load_data(\"data/mnist_train.csv\")\n",
    "X_test,Y_test = load_data(\"data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy to quickly get an idea for how your data looks is to examine the shape of the matrix it's stored using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `60000 x 784`, which tells us there are `60000` samples (images) each with dimension `784`. Each sample, typically presented as a 28 x 28 image, is unrolled into a 1-dimensional vector 28 x 28 = 784 contained within each row of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.720141Z",
     "start_time": "2019-07-15T20:10:03.717400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: 60000 x 784\n",
      "The shape of the test set is: 10000 x 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
    "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take at how the samples are represented, we can do this by calling `Y_train[index]` and `X_train[index]` (here I choose `index=0` to look at the very first sample). We first notice `Y_train[0]=5.0`, indicating this is the digit `5`. We confirm this later by visualizing some of these samples. We then notice each entry is an integer (cast into `np.float32` in our `load_data` function) ranging from `0-255`. This representation is common among image data, the numerical entries are pixel intensities typically shown in gray-scale ranging between `0` (black) and `255` (white). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.766216Z",
     "start_time": "2019-07-15T20:10:03.734027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "         18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "        172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "        253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "        249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "        250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "        221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "          2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "        253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "        172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "        132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "Y_train[index], X_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is nothing in principle wrong with this `0-255` representation, the value of `255` being the maximum is specific to images and somewhat an arbitrary choice in our context. Frequently people opt to rescale their data in these situations to range between `0-1`, which will be have some nice mathematical properties for us later. Of course, this can be done by simply diving each entry in `X_train` and `X_test` by its maximum value (accessed using `X_train.max()`). \n",
    "\n",
    "Further, each label in `Y_train` and `Y_test` are currently integers (e.g. `5.0` or `2.0`). For categorical data (each image must be categorized as label between `0-9`) we opt for a one-hot encoded representation of our data. What this means is each label is converted into a binary vector (e.g. the label 2.0 would be converted to the vector `[0,0,1,0,0,0,0,0,0,0]` and the label 9.0 would be the vector `[0,0,0,0,0,0,0,0,0,1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.883943Z",
     "start_time": "2019-07-15T20:10:03.780726Z"
    }
   },
   "outputs": [],
   "source": [
    "# rescale data between 0 - 1.0\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n",
    "\n",
    "# one-hot encode train (y_train) and test (y_test) set labels\n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size),Y_train.astype(np.int)] = 1.0\n",
    "\n",
    "y_test = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "y_test[np.arange(Y_test.size),Y_test.astype(np.int)] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's go ahead and visualize some of the inputs and their associated labels. We can do this using the `imshow` function (making sure to resize the flattened size 784 representation into a 28 x 28 matrix for compatability with the `imshow` function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:04.313194Z",
     "start_time": "2019-07-15T20:10:03.897501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 4\n",
    "fig,axes = plt.subplots(1,num_images,figsize=(15,10))\n",
    "for image,label,ax in zip(X_train[:num_images],y_train[:num_images],axes):\n",
    "    ax.imshow(image.reshape(28,28),cmap='gray',vmin=0,vmax=1.0)\n",
    "    ax.set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the math \n",
    "Here review some of the math underlying the multilayer perceptron (MLP) and the backpropagation algorithm. The MLP and backpropagation are central to understanding deep learning as a whole. Full stop. What I'm presenting here is by any means *not* an exhaustive exposition of the subject, and I **highly** recommend reading (at least) [chapters 1 & 2 of the free online book: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) for a more complete discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is a multilayer perceptron? Before we answer that question, let's dissect just a singular component: the perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:44:16.088920Z",
     "start_time": "2019-07-15T20:44:15.924981Z"
    }
   },
   "source": [
    "![Perceptron](imgs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image above represents schematically the conventional perceptron. Each input, represented as a multidimensional vector $\\mathbf{x}=\\{x_0=1,x_1,x_2,\\ldots,x_n\\}$, is multiplied by a set of weights $\\mathbf{w} = \\{w_0,w_1,w_2,\\ldots,w_n\\}$ to produce the weighted sum $$z = \\sum_{i=0}^{n} x_i w_i$$. In the case of a vanilla perceptron, this weighted sum $z$ is lastly fed through as step function to produce the predicted output $a = \\text{step}(z)$. This results in the output where $a=1$ if $z>0$ and $a=0$ if $z<0$.\n",
    "\n",
    "With the output $a$ taking the value 0 or 1, this type of structure really only works for binary classification problems (i.e. situations where each input $\\mathbf{x}$ has one of two intended labels: 0 or 1). The weights $\\mathbf{w}$ in this context can be interpreted as assigning credence to the different inputs $x_i$ based on their relative importance. Notice, there is an additional constant $x_0=1$ concatenated to each input, frequently called the *bias term* or just *the bias*. As there is still an associated weight $w_0$ along with this term, this effectively acts as an offset to the origin for the classification. In other words, think of the perceptron classification boundary as a multidimensional line (or plane) $y = m * x + b$. Inputs on one side of the line where $y<0$ get classified as one label ($a=0$), while  inputs on the other side of the line where $y>0$ are classified as the other label ($a=1$). In this geometric interpretation of the perceptron, the weights are like the slope (or vector perpendicular to the plane in higher dimensions) $\\mathbf{w} \\sim m$, and the bias is like the $b$ term $b \\sim w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the perceptron to the multilayer perceptron (MLP) simply introduces more intermediate weighted additions (layers) before the output and generalizes the step function used for binary classification to a broader class of functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](imgs/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neural networks are often schematically represented with a series if nodes and arrows as shown on the right. Each node represents one input, for example the input layer here is shows with 3 inputs, and each arrow represents a *unique* weight given to that particular input in the weighted sum (right side of figure). Note, in our application to MNIST each input would be `784+1` dimensional (including the bias) and the output `10` dimensional for representing the one-hot encoded binary vector of output labels. Rather than restricting ourselves to treating each sum with a step function as in the vanilla perceptron, this concept is generalized to any arbitrary function typically called the *activation function*, depicted as $\\sigma(\\cdot)$ in the schematic. In principle almost any function may be an activation function, however people typically use one of the following activation functions due to some nice mathematical properties they have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation_func](imgs/activation_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:29:56.750472Z",
     "start_time": "2019-07-15T21:29:56.746122Z"
    }
   },
   "source": [
    "Image credit: https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional layers between the inputs an outputs are typically called *hidden layers* because the particular value of the nodes within these layers are hidden to the user. A natural question to ask here is why are hidden layers helpful/necessary for deep learning? The answer is, shockingly, that they aren't necessary, but are very helpful. There exists a theorem called the \"Universal Approximation Theorem\" which states basically that a neural network with a single layer can approximate *any* non-linear function to arbitrary accuracy. While this may imply that only a single hidden layer is, in principle, sufficient for any problem, the necessary dimension of this hidden layer may become intractably large for some problems. Introducing many hidden layers allows your neural network to learn a hierarchy of concepts in the structure of the data. What this amounts to is in a MLP the earlier layers (i.e. the layers closer to the input) learn a coarser representation of the data and begins to more closely reflect the output representation nearer to the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deep](imgs/deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:15:42.928623Z",
     "start_time": "2019-07-15T22:15:42.799116Z"
    }
   },
   "source": [
    "Image credit: UChicago STAT 37710 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward\n",
    "Given a predefined neural network *architecture* (the *architecture* of a neural network refers to all the elements necessary to completely define the flow of data, which involve the number and size of hidden layers, which activation functions, the output size, etc.) the process of generating an output from an input is called a *forward* pass. As we shall see, for an MLP the forward pass may be succinctly represented as a series of matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:19.367434Z",
     "start_time": "2019-07-15T22:16:19.254509Z"
    }
   },
   "source": [
    "![MLP_big](imgs/MLP_big.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:35.149302Z",
     "start_time": "2019-07-15T22:16:35.038196Z"
    }
   },
   "source": [
    "Image credit: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the MLP represented schematically above with sigmoid activations $\\sigma$ in the hidden layer. Each neuron in the hidden layers will be weighted sums of the inputs: $$a_j = \\sigma(\\sum_{i=1}^{784} x_i* w_{i,j})$$ for $j=\\{1,2,\\ldots,15\\}$ (notice, there is no bias term in this example). From here it clear to see that the weights $w_{i,j}$ may be compacted into a matrix $W \\in {\\rm I\\!R}^{784\\times15}$ where $W_{i,j} = w_{i,j}$, allowing for all the neurons in the hidden layer to be efficiently calculated using matrix multiplication: $\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) = \\sigma(\\mathbf{x}W)$. Where $\\mathbf{a}^{(i)},\\mathbf{z}^{(i)} \\in {\\rm I\\!R}^{1\\times 15}$, with the superscript $(i)$ indicating the assocaited layer number, and $\\mathbf{x} \\in {\\rm I\\!R}^{1\\times 784}$ are both arranged as column vectors. Another, different, weight matrix is needed to transform the hidden layer to the output layer. Let's demarcate these two as $W^{(1)}$ for the matrix which transforms the inputs to the hidden layer $\\mathbf{a}^{(1)}$ and $W^{(2)} \\in {\\rm I\\!R}^{15\\times 10}$ for transforming the hidden layer to the output layer $\\mathbf{z}^{(2)}$. Mathematically, $$\\mathbf{z}^{(2)} = \\sigma(\\mathbf{x}W^{(1)})W^{(2)}$$. Notice, we've yet to treat this output $\\mathbf{z}^{(2)}$ with an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:32:36.274844Z",
     "start_time": "2019-07-15T22:32:36.261999Z"
    }
   },
   "source": [
    "The last step of process for classification tasks is actually producing a predictions from these numbers in the output layer $\\mathbf{z}^{(2)}$. Typically, in the case of multi-label classification, this is done using a *softmax* activation function which effectively converts the output neurons into probabilities. This has the mathematical form, $$\\text{softmax}(\\mathbf{z})_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{K=9}\\exp{(z_k)}}$$. The softmax activation function has the property of out outputs summing to 1 $\\sum_{k=1}^{K=9}\\text{softmax}(\\mathbf{z}^{(2)})_k = 1$, allowing each output $\\mathbf{a}^{(2)}_i = \\text{softmax}(\\mathbf{z}^{(2)})_i$ to be interpreted as the probability that the input is actually a digit `0-9`. Note, the output of a MLP does not need to have a softmax activation, for example in a regression setting a softmax activation would not make much sense. When evaluating the classification accuracy of the neural network, the input is typically classified according to the output label with the highest probability, $$\\text{prediction}(\\mathbf{x}) = \\text{argmax}\\  \\text{softmax}(\\mathbf{z}^{(2)}) = \\text{argmax}\\  \\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Now that we have an understanding of how an MLP generates outputs from inputs, we engage the problem of how to actually *train* this neural network. *Training* a neural network (in a supervised setting, which means each input comes with a known output) refers to the process of iteratively updating the weights of the network to improve it's performance. The performance of the neural network is evaluated using a *loss function* which quantitatively measures how \"close\" the neural network output is to the true output. In short, using *backpropagation* we aim to minimize the loss function with respect to the weights (also called *parameters*) of the neural network.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define precicely what we mean by a *cost function*. The cost function $C$ should reflect whatever objective your aim is for the neural network. Here, we're interested in having our neural network predict a digit label `0-9` for each input image. A naturally, and very reasonable, choice in many settings is simply using the L2 norm between our predicted and true outputs $$C = \\frac{1}{2}||\\mathbf{y} - \\mathbf{a}^{(L)}||^2$$, where $y$ represents the true label for input $\\mathbf{x}$ and $\\mathbf{a}^{(L)}$ is the output for the neural network with $L$ layers. Note, in our previous example $\\mathbf{a}^{(L=2)} = \\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$. Of course, when minimizng/maximizing a function any constant multiplies have no effect on the optimum and allowing us to include the $\\frac{1}{2}$ in front as a mathematical convience whose purpose will be clear shortly (spoiler, it has to do with derivatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
